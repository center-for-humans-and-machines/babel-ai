"""Metric models for analysis results and experiment tracking."""

import logging
from datetime import datetime
from typing import Any, Dict, Optional

from pydantic import BaseModel, Field

from models.configs import AgentConfig, ExperimentConfig, FetcherConfig

logger = logging.getLogger(__name__)


class AnalysisResult(BaseModel):
    """Comprehensive analysis results for LLM-generated text outputs.

    This class contains various metrics computed from analyzing LLM responses
    to measure text quality, coherence, similarity, and complexity. It's used
    in drift experiments to track how model outputs change over time.

    Attributes:
        word_count (int): Total number of words in the analyzed text
        unique_word_count (int): Number of unique/distinct words used
        coherence_score (float): Ratio of unique words to total words
            (0.0-1.0). Higher values indicate more diverse vocabulary usage
        lexical_similarity (Optional[float]): Jaccard similarity coefficient
            between current and previous text (0.0-1.0). Measures word overlap
        semantic_similarity (Optional[float]): Cosine similarity between
            sentence embeddings (-1.0 to 1.0). Measures meaning similarity
        lexical_similarity_window (Optional[float]): Average Jaccard similarity
            across analysis window (0.0-1.0)
        semantic_similarity_window (Optional[float]): Average cosine similarity
            across analysis window (-1.0 to 1.0)
        token_perplexity (Optional[float]): Average perplexity of tokens
            (â‰¥1.0). Lower values indicate more predictable/coherent text

    Note:
        Optional fields (lexical_similarity, semantic_similarity,
        lexical_similarity_window, semantic_similarity_window,
        token_perplexity) may be None if comparison data is unavailable
        or if the analysis couldn't be performed.

    Example:
        >>> result = AnalysisResult(
        ...     word_count=45,
        ...     unique_word_count=38,
        ...     coherence_score=0.844,
        ...     lexical_similarity=0.23,
        ...     semantic_similarity=0.78,
        ...     lexical_similarity_window=0.3,
        ...     semantic_similarity_window=0.8,
        ...     token_perplexity=12.5
        ... )
    """

    word_count: int = Field(description="Total number of words in the text")
    unique_word_count: int = Field(description="Number of unique words")
    coherence_score: float = Field(
        description="Ratio of unique words to total words", ge=0.0, le=1.0
    )
    lexical_similarity: Optional[float] = Field(
        None,
        description="Jaccard similarity between current and previous text",
        ge=0.0,
        le=1.0,
    )
    semantic_similarity: Optional[float] = Field(
        None,
        description="Cosine similarity between sentence embeddings",
        ge=-1.0,
        le=1.0,
    )
    lexical_similarity_window: Optional[float] = Field(
        None,
        description="Average Jaccard similarity across analysis window",
        ge=0.0,
        le=1.0,
    )
    semantic_similarity_window: Optional[float] = Field(
        None,
        description="Average cosine similarity across analysis window",
        ge=-1.0,
        le=1.0,
    )
    token_perplexity: Optional[float] = Field(
        None,
        description="Average perplexity of all tokens in the text",
        ge=1.0,
    )


class Metric(BaseModel):
    """A single data point collected during a drift experiment.

    This class represents one measurement taken during an experiment,
    containing both the raw response data and its computed analysis metrics.
    It serves as the fundamental unit of data collection for tracking model
    drift over time.

    Attributes:
        iteration (int): Sequential number of this measurement in the
            experiment. Starts from 0 and increments with each new response
        timestamp (datetime): Exact time when this metric was recorded.
            Used for temporal analysis and experiment tracking
        role (str): The role/type of the message in the conversation
            (typically "system", "user", or "assistant")
        response (str): The actual text content generated by the LLM.
            This is the raw output being analyzed for drift
        analysis (AnalysisResult): Computed metrics for this response
            including word counts, similarity scores, and perplexity measures
        config (Optional[ExperimentConfig]): The experiment configuration
            used when this metric was collected. May be None for space
            efficiency

    Example:
        >>> metric = Metric(
        ...     iteration=42,
        ...     timestamp=datetime.now(),
        ...     role="assistant",
        ...     response="This is a sample response from the model.",
        ...     analysis=AnalysisResult(
        ...         word_count=9,
        ...         unique_word_count=9,
        ...         coherence_score=1.0
        ...     )
        ... )
    """

    iteration: int = Field(description="Iteration number in the experiment")
    timestamp: datetime = Field(description="When this metric was recorded")
    role: str = Field(
        description="Role of the message (system/user/assistant)"
    )
    content: str = Field(description="The actual text response")
    analysis: Optional[AnalysisResult] = Field(
        None, description="Analysis results for this response"
    )

    def to_dict(self) -> Dict[str, Any]:
        """Convert metric to a dictionary format suitable for CSV export.

        Returns:
            Dictionary with flattened structure for CSV export
        """
        logger.info("Converting Metric to dictionary")
        logger.debug(f"Metric: {self.model_dump()}")
        result = {
            "iteration": self.iteration,
            "timestamp": self.timestamp,
            "role": self.role,
            "content": self.content,
            "analysis": self.analysis.model_dump() if self.analysis else None,
        }

        return result


class FetcherMetric(Metric):
    """A single data point collected during a drift experiment."""

    fetcher_config: FetcherConfig = Field(
        description="Configuration of the fetcher that generated the response"
    )

    def to_dict(self) -> Dict[str, Any]:
        """Convert metric to a dictionary format suitable for CSV export."""
        result = super().to_dict()
        logger.info("Updating Metric with fetcher_config.")
        logger.debug(f"Fetcher config: {self.fetcher_config.model_dump()}")
        result.update(
            {
                "fetcher_config": self.fetcher_config.model_dump(),
            }
        )

        return result


class AgentMetric(Metric):
    """A single data point collected during a drift experiment."""

    agent_id: str = Field(
        description="ID of the agent that generated the response"
    )
    agent_config: AgentConfig = Field(
        description="Configuration of the agent that generated the response"
    )

    def to_dict(self) -> Dict[str, Any]:
        """Convert metric to a dictionary format suitable for CSV export."""
        result = super().to_dict()
        logger.info("Updating Metric with agent_id and agent_config.")
        logger.debug(f"Agent id: {self.agent_id}")
        logger.debug(f"Agent config: {self.agent_config.model_dump()}")
        result.update(
            {
                "agent_id": self.agent_id,
                "agent_config": self.agent_config.model_dump(),
            }
        )

        return result


class ExperimentMetadata(BaseModel):
    """Metadata container for LLM drift experiments.

    This class stores comprehensive metadata about an experiment run,
    including timing information, configuration details, and execution
    statistics. It provides a structured way to track and persist
    experiment information for analysis and reproducibility.

    Attributes:
        timestamp: When the experiment was initiated
        config: Complete experiment configuration used
        num_iterations_total: Total number of iterations completed
            (includes both fetcher and agent-generated messages)
        num_fetcher_messages: Number of initial messages from fetcher
        total_characters: Total character count across all messages

    Example:
        >>> config = ExperimentConfig(
        ...     max_iterations=100,
        ...     max_total_characters=10000,
        ...     # ... other config fields
        ... )
        >>>
        >>> metadata = ExperimentMetadata(
        ...     timestamp=datetime.now(),
        ...     config=config,
        ...     num_iterations_total=87,
        ...     num_fetcher_messages=5,
        ...     total_characters=8543
        ... )
    """

    timestamp: datetime
    config: ExperimentConfig
    num_iterations_total: Optional[int] = None
    num_fetcher_messages: Optional[int] = None
    total_characters: Optional[int] = None
