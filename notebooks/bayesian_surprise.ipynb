{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.stats import entropy  # For KL divergence\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "\n",
    "def get_hidden_state_probs(sentence):\n",
    "    \"\"\"Get the probability distribution of hidden states for a sentence.\"\"\"\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "    # Extract last hidden state (shape: [1, seq_len, hidden_dim])\n",
    "    last_hidden_state = outputs.hidden_states[-1].squeeze(0)\n",
    "\n",
    "    # Normalize into probability distribution (softmax over hidden states per token)\n",
    "    probs = F.softmax(last_hidden_state, dim=-1).numpy()\n",
    "    \n",
    "    return probs  # Shape: [seq_len, hidden_dim]\n",
    "\n",
    "def compute_kl_divergence(sentence_before, sentence_after):\n",
    "    \"\"\"Compute KL divergence between hidden state distributions before and after completion.\"\"\"\n",
    "    probs_before = get_hidden_state_probs(sentence_before)\n",
    "    probs_after = get_hidden_state_probs(sentence_after)\n",
    "\n",
    "    # Ensure both distributions have the same number of tokens\n",
    "    min_len = min(probs_before.shape[0], probs_after.shape[0])\n",
    "    probs_before = probs_before[:min_len]\n",
    "    probs_after = probs_after[:min_len]\n",
    "\n",
    "    # Compute KL divergence for each token and take the mean\n",
    "    kl_divs = [entropy(probs_before[i], probs_after[i]) for i in range(min_len)]\n",
    "    avg_kl_div = sum(kl_divs) / len(kl_divs)\n",
    "\n",
    "    return avg_kl_div\n",
    "\n",
    "def calculate_bayesian_surprise(text, target_window_size=100, context_window_size=5):\n",
    "    \"\"\"Calculate Bayesian surprise (KL divergence) at each position in the text using a moving window.\"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    surprises = []\n",
    "    start_words = []\n",
    "\n",
    "    for i in range(len(tokens) - target_window_size - context_window_size):\n",
    "        # Define context window\n",
    "        context = tokens[i : i + target_window_size]  # Take current sliding window\n",
    "        context_str = tokenizer.convert_tokens_to_string(context)\n",
    "\n",
    "        if i + target_window_size < len(tokens):\n",
    "            new_context = tokens[i : i + target_window_size + context_window_size]  # Extend window\n",
    "            new_context_str = tokenizer.convert_tokens_to_string(new_context)\n",
    "\n",
    "            # Compute Bayesian surprise (KL divergence)\n",
    "            kl_div = compute_kl_divergence(context_str, new_context_str)\n",
    "            surprises.append(kl_div)\n",
    "            start_words.append(tokenizer.convert_tokens_to_string([tokens[i+target_window_size+context_window_size]]))  # Track starting word\n",
    "\n",
    "    return start_words, surprises\n",
    "\n",
    "# Example Text\n",
    "text_1 = (\n",
    "    \"user: # The Hobbit: A Tale of Unexpected Courage\\n\\n\"\n",
    "    \"In a hole in the ground there lived a hobbit. Not a dirty, nasty hole, but a cozy hobbit-hole named Bag End, where Bilbo Baggins lived a quiet, respectable life until the wizard Gandalf arrived at his round green door. Though Bilbo prided himself on his predictable routines and comfortable lifestyle, destiny had other plans.\\n\\n\"\n",
    "    \"That evening, thirteen dwarves led by the noble Thorin Oakenshield crowded into his dining room. They spoke of a lost kingdom under the Lonely Mountain, of treasures stolen by the dragon Smaug, and of their desperate quest to reclaim their homeland. Despite all his proper hobbit instincts telling him to stay home, something deep inside Bilbo – a tiny spark of his mother's adventurous Took blood – stirred at their songs of ancient gold and glory.\\n\\n\"\n",
    "    \"Before he knew quite how it happened, Bilbo found himself running down the road without even a pocket handkerchief, joining this band of dwarves as their designated burglar. Through treacherous mountains and dark forests they traveled, facing dangers that Bilbo had never imagined in his comfortable Shire. When trolls captured his companions, it was the small hobbit's quick wit that saved them. Lost in goblin tunnels, he found not only a mysterious ring but also the courage to rely on himself.\\n\\n\"\n",
    "    \"In the shadows of Mirkwood, while giant spiders threatened his friends, Bilbo discovered a bravery he never knew he possessed. No longer was he the same fussy hobbit who had left Bag End – he had grown into someone who would risk everything to help his companions, even when they doubted him.\\n\\n\"\n",
    "    \"Finally reaching the Lonely Mountain, Bilbo faced the terrible dragon Smaug alone. Rather than raw strength, it was his cleverness and compassion that proved most valuable. When he found the Arkenstone – the heart of the mountain that Thorin desperately sought – he made the difficult choice to use it to prevent a war, even though he knew it might cost him his friendship with Thorin.\\n\\n\"\n",
    "    \"In the great battle that followed, Bilbo witnessed the terrible cost of greed as Thorin finally overcame his dragon-sickness, but too late to prevent tragedy. Yet even in darkness, the hobbit's fundamental decency and loyalty never wavered. He stayed by Thorin's side until the end, learning that there are indeed many kinds of courage, and that the smallest person can change the course of the future.\\n\\n\"\n",
    "    \"When at last Bilbo returned to Bag End, he was no longer quite respectable by hobbit standards. But he had gained something far more valuable – wisdom, courage, and stories worth telling. His modest hobbit hole was the same, but he was not, having learned that true wealth lies not in what you keep safely locked away, but in what you're willing to risk for friendship and what's right.\\n\\n\"\n",
    "    \"The simple hobbit who had never ventured beyond his garden had become a legend, proving that greatness can come from the most unexpected places, and that the most important journeys change us in ways we never imagined.\\n\\n\"\n",
    "    \"\\nPlease ponder the question the user asks together with him.\\n--------------\\n\"\n",
    "    \"user: I heard there is a new adventure in town. Do you think we should go on it?\\n--------------\\n\"\n",
    "    \"assistant: Embarking on a new adventure can be exciting and transformative, much like Bilbo's journey in \\\"The Hobbit: A Tale of Unexpected Courage.\\\" Let's ponder a few aspects together:\\n\\n\"\n",
    "    \"1. **Desire for Change:** Like Bilbo, is there a part of you intrigued by the unknown or seeking change from your daily routine? Sometimes curiosity can lead to great personal growth.\\n\\n\"\n",
    "    \"2. **Potential Challenges:** Adventures often come with unexpected challenges. Are you prepared to face difficulties as they arise and handle them with resilience?\\n\\n\"\n",
    "    \"3. **Personal Growth:** Consider how this adventure might help you learn more about yourself, develop new skills, or gain fresh perspectives.\\n\\n\"\n",
    "    \"4. **Impact on Others:** Think about whether there are others who might benefit from your decision to embark on this journey—much like how Bilbo's choice impacted his friends and the broader world.\\n\\n\"\n",
    "    \"5. **Safety and Comfort Zones:** It's important to ensure that the adventure aligns with your comfort level regarding safety and well-being but also encourages stepping slightly out of comfort zones for meaningful experiences.\\n\\n\"\n",
    "    \"6. **Purpose and Goals:** Reflect on what you hope to achieve from this adventure—new friendships, stories worth sharing, or perhaps just a break from routine—and whether it aligns with your values.\\n\\n\"\n",
    "    \"Ultimately, going on an adventure is a personal decision that involves weighing both the excitement of discovery against any potential risks or sacrifices involved. Whether you're inspired by Bilbo's courage or simply curious about what's out there, trust in your instincts while staying true to what feels right for you personally at this time!\\n--------------\\n\"\n",
    "    \"user: Embarking on a new adventure indeed presents an invigorating prospect, much like Bilbo's transformative journey. Let's explore these aspects more deeply:\\n\\n\"\n",
    "    \"1. **Desire for Change:** If you feel a pull towards the unknown or find yourself seeking a break from routine, that's worth listening to. Curiosity often sparks the beginning of a journey that can lead to unforeseen personal growth and satisfaction.\\n\\n\"\n",
    "    \"2. **Potential Challenges:** No adventure is without its hurdles. Consider whether you're ready to face such challenges head-on and adapt as needed. Remember, resilience can be your greatest ally during uncertain times.\\n\\n\"\n",
    "    \"3. **Personal Growth:** An adventure can teach you much about yourself, offering opportunities to expand your skills and perspectives in ways everyday life might not allow.\\n\\n\"\n",
    "    \"4. **Impact on Others:** Your decision might resonate beyond your own experiences—like how Bilbo's journey affected those around him—possibly inspiring others or bringing about positive change within your community.\\n\\n\"\n",
    "    \"5. **Safety and Comfort Zones:** Balancing safety with stepping out of comfort zones is crucial for meaningful experiences while ensuring well-being isn't compromised.\\n\\n\"\n",
    "    \"6. **Purpose and Goals:** Clarify what you wish to achieve through this adventure, whether it's building new relationships, creating memorable stories, or simply taking time away from the mundane routine—and ensure these aims align with personal values.\\n\\n\"\n",
    "    \"Ultimately, deciding to embark on an adventure is deeply personal and involves weighing excitement against potential risks or sacrifices involved. Whether inspired by tales like Bilbo's courage-filled expedition or driven by sheer curiosity about what lies beyond familiar horizons, trust yourself while staying true to what feels right at this moment!\\n--------------\\n\"\n",
    ")\n",
    "    \n",
    "text_2 = (\n",
    "    \"Lena had always been close to her grandmother, who lived in a cozy cottage at \"\n",
    "    \"the edge of the forest. One evening, she received a letter from her grandmother \"\n",
    "    \"asking her to visit urgently. It had been weeks since their last conversation, \"\n",
    "    \"and Lena was curious about what could be so important. She packed a bag and \"\n",
    "    \"made her way through the dense woods, where shadows seemed to play tricks on \"\n",
    "    \"her mind. As she approached the cottage, the door was slightly ajar.\\n\\n\"\n",
    "    \n",
    "    \"Inside, everything appeared as usual, with the scent of lavender filling the \"\n",
    "    \"air. \\\"Grandma?\\\" Lena called, but there was no answer. She ventured further \"\n",
    "    \"in, and to her surprise, the living room was empty. A single note was placed \"\n",
    "    \"on the table, reading: *\\\"Look behind the painting.\\\"*\\n\\n\"\n",
    "    \n",
    "    \"Lena's heart skipped. She had always been told not to meddle with the old \"\n",
    "    \"paintings in the house. But curiosity got the best of her, and she gently \"\n",
    "    \"moved the painting. To her shock, there was a hidden door behind it. Hesitant \"\n",
    "    \"but determined, Lena opened the door and found a staircase leading down into \"\n",
    "    \"darkness.\\n\\n\"\n",
    "    \n",
    "    \"As she descended, she felt an icy chill in the air. The basement was far \"\n",
    "    \"larger than she expected, and it was filled with old trunks and mysterious \"\n",
    "    \"relics. Suddenly, she heard a creak from behind. Turning, she was startled to \"\n",
    "    \"see her grandmother standing there, grinning strangely. \\\"Grandma, what's \"\n",
    "    \"going on?\\\" Lena asked, trembling.\\n\\n\"\n",
    "    \n",
    "    \"The woman chuckled softly. \\\"I suppose you're wondering about the truth.\\\" \"\n",
    "    \"Lena was confused. \\\"What truth?\\\"\\n\\n\"\n",
    "    \n",
    "    \"Her grandmother stepped closer. \\\"I'm not your grandmother,\\\" she said coldly. \"\n",
    "    \"Lena gasped. The woman in front of her wasn't her grandmother at all. But \"\n",
    "    \"before Lena could react, the woman reached into her pocket and pulled out a \"\n",
    "    \"small locket. She opened it, revealing a picture of Lena's real grandmother, \"\n",
    "    \"smiling beside a young girl.\\n\\n\"\n",
    "    \n",
    "    \"With a shock, Lena realized the girl was herself—*as a child*. But that \"\n",
    "    \"wasn't possible. She hadn't been here in years, and that was when she \"\n",
    "    \"understood. The woman in front of her wasn't just a stranger; she was her \"\n",
    "    \"long-lost twin sister.\"\n",
    ")\n",
    "\n",
    "# Compute Bayesian surprise\n",
    "start_words, bayesian_surprises = calculate_bayesian_surprise(text_1, target_window_size=100, context_window_size=10)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(100, 5))\n",
    "plt.plot(range(len(bayesian_surprises)), bayesian_surprises, marker='o', linestyle='-', color='b')\n",
    "plt.xticks(range(len(start_words)), start_words, rotation=90)\n",
    "plt.axvline(x=0, color='gray', alpha=0.3, linewidth=1.5)  # Add vertical line at x=0\n",
    "for x in range(1, len(start_words)):\n",
    "    plt.axvline(x=x, color='gray', alpha=0.3, linewidth=1.5)\n",
    "plt.xlabel(\"Starting Word of Context Window\")\n",
    "plt.ylabel(\"Bayesian Surprise (KL Divergence)\")\n",
    "plt.title(\"Bayesian Surprise Across Text Positions\")\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.figure(figsize=(200, 5))\n",
    "plt.plot(range(len(bayesian_surprises)), bayesian_surprises, marker='o', linestyle='-', color='b')\n",
    "plt.xticks(range(len(start_words)), start_words, rotation=90)\n",
    "plt.axvline(x=0, color='gray', alpha=0.3, linewidth=1.5)  # Add vertical line at x=0\n",
    "for x in range(1, len(start_words)):\n",
    "    plt.axvline(x=x, color='gray', alpha=0.3, linewidth=1.5)\n",
    "plt.xlabel(\"Starting Word of Context Window\")\n",
    "plt.ylabel(\"Bayesian Surprise (KL Divergence)\")\n",
    "plt.title(\"Bayesian Surprise Across Text Positions\")\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def get_sentence_embedding(sentence):\n",
    "    \"\"\"Get the BERT embedding for the CLS token of a sentence.\"\"\"\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract the [CLS] token embedding (batch_size=1, seq_len, hidden_dim)\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]  # Shape: (1, hidden_dim)\n",
    "    \n",
    "    return cls_embedding.squeeze(0).numpy()  # Convert to NumPy for distance calculation\n",
    "\n",
    "def compute_embedding_shift(sentence_before, sentence_after):\n",
    "    \"\"\"Compute cosine similarity between sentence embeddings before and after completion.\"\"\"\n",
    "    emb_before = get_sentence_embedding(sentence_before)\n",
    "    emb_after = get_sentence_embedding(sentence_after)\n",
    "\n",
    "    similarity = 1 - cosine(emb_before, emb_after)  # Cosine similarity (higher = more similar)\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "# Example sentences\n",
    "sentence_before = \"I pack my bathing suit and go to the\"\n",
    "sentence_after_1 = \"I pack my bathing suit and go to the swimming pool.\"\n",
    "sentence_after_2 = \"I pack my bathing suit and go to the courtroom to defend myself.\"\n",
    "\n",
    "# Compute semantic similarity scores\n",
    "similarity_1 = compute_embedding_shift(sentence_before, sentence_after_1)\n",
    "similarity_2 = compute_embedding_shift(sentence_before, sentence_after_2)\n",
    "\n",
    "print(f\"Semantic Similarity (swimming pool completion): {similarity_1:.4f} (high similarity)\")\n",
    "print(f\"Semantic Similarity (courtroom completion): {similarity_2:.4f} (low similarity)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
    "\n",
    "def get_attention(sentence, max_len):\n",
    "    \"\"\"Compute mean attention across layers & heads with padding to match max sequence length.\"\"\"\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=max_len)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    attentions = torch.stack(outputs.attentions)  # Shape: (num_layers, num_heads, seq_len, seq_len)\n",
    "    \n",
    "    return attentions.mean(dim=(0, 1))  # Average over layers & heads → Shape: (seq_len, seq_len)\n",
    "\n",
    "# Example sentences\n",
    "before = \"I pack my bathing suit and go to the\"\n",
    "after1 = \"I pack my bathing suit and go to the swimming pool.\"\n",
    "after2 = \"I pack my bathing suit and go to the courtroom to defend myself.\"\n",
    "\n",
    "# Determine max sequence length for padding\n",
    "max_seq_len = max(len(tokenizer.tokenize(before)), \n",
    "                  len(tokenizer.tokenize(after1)), \n",
    "                  len(tokenizer.tokenize(after2))) + 2  # +2 for [CLS] and [SEP] tokens\n",
    "\n",
    "# Compute attention matrices with padding\n",
    "attn_before = get_attention(before, max_seq_len)\n",
    "attn_after1 = get_attention(after1, max_seq_len)\n",
    "attn_after2 = get_attention(after2, max_seq_len)\n",
    "\n",
    "# Compare attention reallocation\n",
    "shift1 = (attn_after1 - attn_before).abs().sum().item()  # Low impact\n",
    "shift2 = (attn_after2 - attn_before).abs().sum().item()  # High impact\n",
    "\n",
    "print(f\"Attention shift (swimming pool): {shift1:.4f} (low impact novelty)\")\n",
    "print(f\"Attention shift (courtroom): {shift2:.4f} (high impact novelty)\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
