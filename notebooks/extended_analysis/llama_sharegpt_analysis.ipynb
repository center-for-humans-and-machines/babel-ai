{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mienhardt/Programming/babel_ai/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from babel_ai.prompt_fetcher import ShareGPTPromptFetcher\n",
    "from babel_ai.models import ExperimentConfig\n",
    "from babel_ai.llm_drift import DriftExperiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mienhardt/Programming/babel_ai\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "cwd = Path.cwd().parent.parent\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing initial messages:   0%|          | 0/58 [00:00<?, ?it/s]Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Analyzing initial messages:   7%|▋         | 4/58 [00:00<00:01, 35.38it/s]Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Analyzing initial messages:  17%|█▋        | 10/58 [00:00<00:00, 49.07it/s]Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Analyzing initial messages:  28%|██▊       | 16/58 [00:00<00:00, 53.12it/s]Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Analyzing initial messages:  40%|███▉      | 23/58 [00:00<00:00, 57.43it/s]Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Analyzing initial messages:  52%|█████▏    | 30/58 [00:00<00:00, 58.97it/s]Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Analyzing initial messages:  66%|██████▌   | 38/58 [00:00<00:00, 62.67it/s]Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Analyzing initial messages:  78%|███████▊  | 45/58 [00:00<00:00, 63.37it/s]Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Analyzing initial messages:  91%|█████████▏| 53/58 [00:00<00:00, 66.28it/s]Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Input text has only one token. Perplexity calculation requires at least two tokens. Returning max perplexity.\n",
      "Analyzing initial messages: 100%|██████████| 58/58 [00:00<00:00, 61.52it/s]\n",
      "Running drift experiment:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running drift experiment:   2%|▏         | 1/50 [00:32<26:08, 32.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running drift experiment:   4%|▍         | 2/50 [00:44<16:17, 20.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running drift experiment:   6%|▌         | 3/50 [00:55<12:39, 16.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running drift experiment:   8%|▊         | 4/50 [01:05<10:36, 13.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running drift experiment:  10%|█         | 5/50 [01:15<09:19, 12.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running drift experiment:  12%|█▏        | 6/50 [01:29<09:33, 13.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running drift experiment:  14%|█▍        | 7/50 [01:46<10:06, 14.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running drift experiment:  16%|█▌        | 8/50 [02:06<11:18, 16.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running drift experiment:  18%|█▊        | 9/50 [02:35<13:40, 20.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running drift experiment:  20%|██        | 10/50 [02:59<14:19, 21.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running drift experiment:  22%|██▏       | 11/50 [03:29<15:37, 24.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running drift experiment:  26%|██▌       | 13/50 [03:56<11:32, 18.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running drift experiment:  30%|███       | 15/50 [04:28<10:20, 17.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running drift experiment:  32%|███▏      | 16/50 [04:39<08:46, 15.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running drift experiment:  34%|███▍      | 17/50 [05:09<11:01, 20.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running drift experiment:  36%|███▌      | 18/50 [05:32<11:07, 20.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running drift experiment:  38%|███▊      | 19/50 [06:16<14:20, 27.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running drift experiment:  40%|████      | 20/50 [06:43<13:49, 27.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1220 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Running drift experiment:  40%|████      | 20/50 [07:39<11:29, 22.99s/it]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 27\u001b[0m\n\u001b[1;32m     20\u001b[0m drift_experiment \u001b[38;5;241m=\u001b[39m DriftExperiment(\n\u001b[1;32m     21\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m     22\u001b[0m     prompt_fetcher\u001b[38;5;241m=\u001b[39mprompt_fetcher,\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m initial_messages \u001b[38;5;241m=\u001b[39m drift_experiment\u001b[38;5;241m.\u001b[39mprompt_fetcher\u001b[38;5;241m.\u001b[39mget_random_prompt()\n\u001b[0;32m---> 27\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mdrift_experiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_messages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_messages\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/babel_ai/src/babel_ai/llm_drift.py:171\u001b[0m, in \u001b[0;36mDriftExperiment.run\u001b[0;34m(self, initial_messages)\u001b[0m\n\u001b[1;32m    168\u001b[0m msgs_text\u001b[38;5;241m.\u001b[39mappend(response)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Analyze all outputs\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m analysis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsgs_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# Store results\u001b[39;00m\n\u001b[1;32m    174\u001b[0m metrics\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    175\u001b[0m     Metric(\n\u001b[1;32m    176\u001b[0m         iteration\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(msgs_text),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m     )\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m~/Programming/babel_ai/src/babel_ai/analyzer.py:237\u001b[0m, in \u001b[0;36mSimilarityAnalyzer.analyze\u001b[0;34m(self, outputs)\u001b[0m\n\u001b[1;32m    232\u001b[0m     semantic_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_analyze_semantic_similarity(\n\u001b[1;32m    233\u001b[0m         current_text, previous_texts\n\u001b[1;32m    234\u001b[0m     )\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# Get token perplexity metrics\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m token_perplexity_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_analyze_token_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m AnalysisResult(\n\u001b[1;32m    240\u001b[0m     word_stats\u001b[38;5;241m=\u001b[39mword_stats,\n\u001b[1;32m    241\u001b[0m     lexical\u001b[38;5;241m=\u001b[39mlexical_metrics,\n\u001b[1;32m    242\u001b[0m     semantic\u001b[38;5;241m=\u001b[39msemantic_metrics,\n\u001b[1;32m    243\u001b[0m     token_perplexity\u001b[38;5;241m=\u001b[39mtoken_perplexity_metrics,\n\u001b[1;32m    244\u001b[0m )\n",
      "File \u001b[0;32m~/Programming/babel_ai/src/babel_ai/analyzer.py:199\u001b[0m, in \u001b[0;36mSimilarityAnalyzer._analyze_token_perplexity\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m perplexity\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# Get perplexity for current text\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m avg_perplexity \u001b[38;5;241m=\u001b[39m \u001b[43mget_token_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m TokenPerplexityMetrics(\n\u001b[1;32m    202\u001b[0m     avg_token_perplexity\u001b[38;5;241m=\u001b[39mavg_perplexity,\n\u001b[1;32m    203\u001b[0m )\n",
      "File \u001b[0;32m~/Programming/babel_ai/src/babel_ai/analyzer.py:178\u001b[0m, in \u001b[0;36mSimilarityAnalyzer._analyze_token_perplexity.<locals>.get_token_perplexity\u001b[0;34m(input_text)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# Get model predictions\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 178\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits[\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;241m0\u001b[39m, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :\n\u001b[1;32m    181\u001b[0m     ]  \u001b[38;5;66;03m# Shape: [seq_len-1, vocab_size]\u001b[39;00m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Get target tokens (shifted by 1)\u001b[39;00m\n",
      "File \u001b[0;32m~/Programming/babel_ai/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/babel_ai/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Programming/babel_ai/.venv/lib/python3.13/site-packages/transformers/models/gpt2/modeling_gpt2.py:1062\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1062\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1077\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/Programming/babel_ai/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/babel_ai/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Programming/babel_ai/.venv/lib/python3.13/site-packages/transformers/models/gpt2/modeling_gpt2.py:820\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    819\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwte(input_ids)\n\u001b[0;32m--> 820\u001b[0m position_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m position_embeds\u001b[38;5;241m.\u001b[39mto(inputs_embeds\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# Attention mask.\u001b[39;00m\n",
      "File \u001b[0;32m~/Programming/babel_ai/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/babel_ai/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Programming/babel_ai/.venv/lib/python3.13/site-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/babel_ai/.venv/lib/python3.13/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "config = ExperimentConfig(\n",
    "    provider=\"raven\",\n",
    "    model=\"llama3.3:70b\",\n",
    "    temperature=1,\n",
    "    max_tokens=2000,\n",
    "    frequency_penalty=2.0,\n",
    "    presence_penalty=2.0,\n",
    "    top_p=1.0,\n",
    "    max_iterations=50,\n",
    "    max_total_characters=1000000,\n",
    "    analyze_window=50,\n",
    ")\n",
    "\n",
    "prompt_fetcher = ShareGPTPromptFetcher(\n",
    "    data_path=cwd / 'data' / 'human-ai_datasets' / 'sharegpt_clean.json',\n",
    "    min_messages=50,\n",
    "    max_messages=100,\n",
    ")\n",
    "\n",
    "drift_experiment = DriftExperiment(\n",
    "    config=config,\n",
    "    prompt_fetcher=prompt_fetcher,\n",
    ")\n",
    "\n",
    "initial_messages = drift_experiment.prompt_fetcher.get_random_prompt()\n",
    "\n",
    "metrics = drift_experiment.run(\n",
    "    initial_messages=initial_messages\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
