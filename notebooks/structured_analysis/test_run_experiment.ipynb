{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c549fc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test configuration from YAML file\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set working directory to project root\n",
    "project_root = Path.cwd().parent.parent\n",
    "print(f\"Current working directory: {Path.cwd()}\")\n",
    "print(f\"Setting working directory to project root: {project_root}\")\n",
    "os.chdir(project_root)\n",
    "print(f\"New working directory: {Path.cwd()}\")\n",
    "\n",
    "# Add src to path so we can import the modules\n",
    "sys.path.append(str(Path.cwd() / \"src\"))\n",
    "\n",
    "from utils import load_yaml_config\n",
    "from models import ExperimentConfig\n",
    "from babel_ai.experiment import Experiment\n",
    "\n",
    "# Load the test configuration\n",
    "config_path = \"configs/test_config.yaml\"\n",
    "config = load_yaml_config(ExperimentConfig, config_path)\n",
    "\n",
    "print(\"Loaded configuration:\")\n",
    "print(f\"- Fetcher: {config.fetcher_config.fetcher}\")\n",
    "print(f\"- Analyzer: {config.analyzer_config.analyzer}\")\n",
    "print(f\"- Max iterations: {config.max_iterations}\")\n",
    "print(f\"- Max total characters: {config.max_total_characters}\")\n",
    "print(f\"- Output directory: {config.output_dir}\")\n",
    "print(f\"- Agent configs: {len(config.agent_configs)} agent(s)\")\n",
    "\n",
    "# Display the configuration\n",
    "config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168239aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the configuration for testing\n",
    "from models import AgentConfig, FetcherConfig, AnalyzerConfig\n",
    "from babel_ai.enums import FetcherType, AnalyzerType, AgentSelectionMethod\n",
    "from api.enums import AzureModels, OpenAIModels, Provider\n",
    "\n",
    "# Create a modified configuration with smaller limits for testing\n",
    "# and use random fetcher instead of sharegpt (which requires data files)\n",
    "modified_config = ExperimentConfig(\n",
    "    fetcher_config=FetcherConfig(\n",
    "        fetcher=FetcherType.TOPICAL_CHAT,\n",
    "        data_path=\"data/human_human_dataset/Topical-Chat/test_freq.jsonl\",\n",
    "        second_data_path=\"data/human_human_dataset/Topical-Chat/test_rare.jsonl\",\n",
    "        min_messages=5,\n",
    "        max_messages=50,\n",
    "    ),\n",
    "    analyzer_config=AnalyzerConfig(\n",
    "        analyzer=AnalyzerType.SIMILARITY,\n",
    "        analyze_window=5  # Smaller window for testing\n",
    "    ),\n",
    "    agent_configs=[\n",
    "        AgentConfig(\n",
    "            provider=Provider.AZURE,\n",
    "            model=AzureModels.GPT4O_2024_08_06,  # Use smaller model for testing\n",
    "            system_prompt=\"You are a helpful assistant in a conversation.\",\n",
    "            temperature=0.8,\n",
    "            max_tokens=100,  # Smaller for testing\n",
    "            frequency_penalty=0.0,\n",
    "            presence_penalty=0.0,\n",
    "            top_p=1.0\n",
    "        ),\n",
    "        AgentConfig(\n",
    "            provider=Provider.AZURE,\n",
    "            model=AzureModels.GPT4O_2024_08_06,  # Use smaller model for testing\n",
    "            system_prompt=\"Your task is to keep the conversation interesting and develop new angles.\",\n",
    "            temperature=1.2,\n",
    "            max_tokens=100,  # Smaller for testing\n",
    "            frequency_penalty=0.0,\n",
    "            presence_penalty=0.0,\n",
    "            top_p=1.0\n",
    "        ),\n",
    "    ],\n",
    "    agent_selection_method=AgentSelectionMethod.ROUND_ROBIN,\n",
    "    max_iterations=35,  # Much smaller for testing\n",
    "    max_total_characters=100000,  # Smaller limit\n",
    "    output_dir=\"data/test_results\"\n",
    ")\n",
    "\n",
    "print(\"Modified configuration:\")\n",
    "print(f\"- Fetcher: {modified_config.fetcher_config.fetcher}\")\n",
    "print(f\"- Fetcher category: {modified_config.fetcher_config.category}\")\n",
    "print(f\"- Analyzer window: {modified_config.analyzer_config.analyze_window}\")\n",
    "print(f\"- Model: {modified_config.agent_configs[0].model}\")\n",
    "print(f\"- Max iterations: {modified_config.max_iterations}\")\n",
    "print(f\"- Max tokens per response: {modified_config.agent_configs[0].max_tokens}\")\n",
    "print(f\"- Output directory: {modified_config.output_dir}\")\n",
    "\n",
    "modified_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d313948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment with modified configuration\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up logging to see what's happening\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "print(\"Starting experiment...\")\n",
    "print(f\"Timestamp: {datetime.now()}\")\n",
    "\n",
    "# Create and run the experiment\n",
    "try:\n",
    "    # Create experiment instance\n",
    "    experiment = Experiment(\n",
    "        config=modified_config,\n",
    "        use_notebook_tqdm=True  # Use notebook-friendly progress bars\n",
    "    )\n",
    "    \n",
    "    print(f\"Experiment created successfully!\")\n",
    "    print(f\"Output directory: {experiment.output_dir}\")\n",
    "    print(f\"Initial messages from fetcher: {len(experiment.messages)}\")\n",
    "    \n",
    "    # Run the experiment\n",
    "    print(\"\\nRunning experiment...\")\n",
    "    results = experiment.run()\n",
    "    \n",
    "    print(f\"\\nExperiment completed!\")\n",
    "    print(f\"Total results: {len(results)}\")\n",
    "    print(f\"Final message count: {len(experiment.messages)}\")\n",
    "    print(f\"Total characters: {experiment.total_characters}\")\n",
    "    \n",
    "    # Show summary of results\n",
    "    print(\"\\nResults summary:\")\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"  {i+1}. {result.role}: {result.content[:60]}...\")\n",
    "    \n",
    "    # Store results for next cell\n",
    "    experiment_results = results\n",
    "    output_directory = experiment.output_dir\n",
    "    \n",
    "    print(f\"\\nResults saved to: {output_directory}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error running experiment: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dc8db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the output files and create visualizations\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Examining output files...\")\n",
    "\n",
    "try:\n",
    "    # Ensure output_directory is a Path object\n",
    "    if 'output_directory' not in locals():\n",
    "        output_directory = Path(\"data/test_results\")\n",
    "    else:\n",
    "        output_directory = Path(output_directory)\n",
    "    \n",
    "    print(f\"Looking in directory: {output_directory}\")\n",
    "    \n",
    "    # Find the CSV and JSON files\n",
    "    csv_files = list(output_directory.glob(\"*.csv\"))\n",
    "    json_files = list(output_directory.glob(\"*_meta.json\"))\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files and {len(json_files)} JSON files\")\n",
    "    \n",
    "    if json_files:\n",
    "        # Load metadata (use the most recent one)\n",
    "        json_file = sorted(json_files)[-1]\n",
    "        print(f\"Using metadata file: {json_file.name}\")\n",
    "        \n",
    "        with open(json_file, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        # Display relevant metadata\n",
    "        print(\"=== EXPERIMENT METADATA ===\")\n",
    "        print(f\"Timestamp: {metadata.get('timestamp', 'N/A')}\")\n",
    "        print()\n",
    "        \n",
    "        # Fetcher information\n",
    "        fetcher_config = metadata.get('config', {}).get('fetcher_config', {})\n",
    "        print(f\"Fetcher Type: {fetcher_config.get('fetcher', 'N/A')}\")\n",
    "        print(f\"Number of Fetched Messages: {metadata.get('num_fetcher_messages', 'N/A')}\")\n",
    "        print()\n",
    "        \n",
    "        # Analyzer information\n",
    "        analyzer_config = metadata.get('config', {}).get('analyzer_config', {})\n",
    "        print(f\"Analyzer Type: {analyzer_config.get('analyzer', 'N/A')}\")\n",
    "        print(f\"Analyzer Window: {analyzer_config.get('analyze_window', 'N/A')}\")\n",
    "        print()\n",
    "        \n",
    "        # Agent information\n",
    "        agent_configs = metadata.get('config', {}).get('agent_configs', [])\n",
    "        print(\"Agent Data:\")\n",
    "        for i, agent in enumerate(agent_configs):\n",
    "            print(f\"  Agent {i+1}:\")\n",
    "            print(f\"    Provider: {agent.get('provider', 'N/A')}\")\n",
    "            print(f\"    Model: {agent.get('model', 'N/A')}\")\n",
    "            print(f\"    Temperature: {agent.get('temperature', 'N/A')}\")\n",
    "            print(f\"    Max Tokens: {agent.get('max_tokens', 'N/A')}\")\n",
    "            print(f\"    Frequency Penalty: {agent.get('frequency_penalty', 'N/A')}\")\n",
    "            print(f\"    Presence Penalty: {agent.get('presence_penalty', 'N/A')}\")\n",
    "            print(f\"    Top P: {agent.get('top_p', 'N/A')}\")\n",
    "            print()\n",
    "        \n",
    "        # Selection method and totals\n",
    "        config = metadata.get('config', {})\n",
    "        print(f\"Agent Selection Method: {config.get('agent_selection_method', 'N/A')}\")\n",
    "        print(f\"Total Iterations: {metadata.get('num_iterations_total', 'N/A')}\")\n",
    "        print(f\"Total Characters: {metadata.get('total_characters', 'N/A')}\")\n",
    "        \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    # Load and visualize analysis results\n",
    "    if csv_files:\n",
    "        print(\"=== ANALYSIS RESULTS VISUALIZATION ===\")\n",
    "        \n",
    "        # Use the most recent CSV file\n",
    "        csv_file = sorted(csv_files)[-1]\n",
    "        print(f\"Using CSV file: {csv_file.name}\")\n",
    "        \n",
    "        df = pd.read_csv(csv_file)\n",
    "        print(f\"Loaded DataFrame with shape: {df.shape}\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Parse the analysis column which contains JSON strings\n",
    "        analysis_data = []\n",
    "        for idx, row in df.iterrows():\n",
    "            try:\n",
    "                if pd.notna(row['analysis']):\n",
    "                    analysis_dict = ast.literal_eval(row['analysis'])\n",
    "                    analysis_data.append(analysis_dict)\n",
    "                else:\n",
    "                    analysis_data.append({})\n",
    "            except:\n",
    "                analysis_data.append({})\n",
    "        \n",
    "        # Create a DataFrame from the analysis data\n",
    "        analysis_df = pd.DataFrame(analysis_data)\n",
    "        print(f\"Extracted analysis metrics: {list(analysis_df.columns)}\")\n",
    "        \n",
    "        # Add basic metrics\n",
    "        df['response_length'] = df['content'].str.len()\n",
    "        df['cumulative_chars'] = df['response_length'].cumsum()\n",
    "        \n",
    "        # Merge analysis metrics\n",
    "        for col in analysis_df.columns:\n",
    "            df[col] = analysis_df[col]\n",
    "        \n",
    "        # Set up the plotting style\n",
    "        plt.style.use('default')\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle('Drift Analysis Results', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Get the separation point between fetched and generated messages\n",
    "        num_fetcher_messages = metadata.get('num_fetcher_messages', 0)\n",
    "        separation_point = num_fetcher_messages - 0.5\n",
    "        \n",
    "        # Plot 1: Lexical similarity over iterations\n",
    "        if 'lexical_similarity' in df.columns and df['lexical_similarity'].notna().any():\n",
    "            lexical_data = df['lexical_similarity'].dropna()\n",
    "            axes[0, 0].plot(lexical_data.index, lexical_data.values, \n",
    "                           marker='o', linewidth=2, markersize=6, color='blue')\n",
    "            axes[0, 0].axvline(x=separation_point, color='red', linestyle='--', \n",
    "                              alpha=0.7, label='Fetched | Generated')\n",
    "            axes[0, 0].set_title('Lexical Similarity Over Iterations')\n",
    "            axes[0, 0].set_xlabel('Iteration')\n",
    "            axes[0, 0].set_ylabel('Lexical Similarity')\n",
    "            axes[0, 0].grid(True, alpha=0.3)\n",
    "            axes[0, 0].legend()\n",
    "        else:\n",
    "            axes[0, 0].text(0.5, 0.5, 'No lexical similarity data\\navailable', \n",
    "                           ha='center', va='center', transform=axes[0, 0].transAxes)\n",
    "            axes[0, 0].set_title('Lexical Similarity Over Iterations')\n",
    "        \n",
    "        # Plot 2: Semantic similarity over iterations\n",
    "        if 'semantic_similarity' in df.columns and df['semantic_similarity'].notna().any():\n",
    "            semantic_data = df['semantic_similarity'].dropna()\n",
    "            axes[0, 1].plot(semantic_data.index, semantic_data.values, \n",
    "                           marker='s', linewidth=2, markersize=6, color='green')\n",
    "            axes[0, 1].axvline(x=separation_point, color='red', linestyle='--', \n",
    "                              alpha=0.7, label='Fetched | Generated')\n",
    "            axes[0, 1].set_title('Semantic Similarity Over Iterations')\n",
    "            axes[0, 1].set_xlabel('Iteration')\n",
    "            axes[0, 1].set_ylabel('Semantic Similarity')\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "            axes[0, 1].legend()\n",
    "        else:\n",
    "            axes[0, 1].text(0.5, 0.5, 'No semantic similarity data\\navailable', \n",
    "                           ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "            axes[0, 1].set_title('Semantic Similarity Over Iterations')\n",
    "        \n",
    "        # Plot 3: Semantic similarity window over iterations\n",
    "        if 'semantic_similarity_window' in df.columns and df['semantic_similarity_window'].notna().any():\n",
    "            semantic_window_data = df['semantic_similarity_window'].dropna()\n",
    "            axes[1, 0].plot(semantic_window_data.index, semantic_window_data.values, \n",
    "                           marker='^', linewidth=2, markersize=6, color='purple')\n",
    "            axes[1, 0].axvline(x=separation_point, color='red', linestyle='--', \n",
    "                              alpha=0.7, label='Fetched | Generated')\n",
    "            axes[1, 0].set_title('Semantic Similarity Window Over Iterations')\n",
    "            axes[1, 0].set_xlabel('Iteration')\n",
    "            axes[1, 0].set_ylabel('Semantic Similarity Window')\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "            axes[1, 0].legend()\n",
    "        else:\n",
    "            axes[1, 0].text(0.5, 0.5, 'No semantic similarity window\\ndata available', \n",
    "                           ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "            axes[1, 0].set_title('Semantic Similarity Window Over Iterations')\n",
    "        \n",
    "        # Plot 4: Token perplexity over iterations\n",
    "        if 'token_perplexity' in df.columns and df['token_perplexity'].notna().any():\n",
    "            perplexity_data = df['token_perplexity'].dropna()\n",
    "            axes[1, 1].plot(perplexity_data.index, perplexity_data.values, \n",
    "                           marker='d', linewidth=2, markersize=4, color='orange')\n",
    "            axes[1, 1].axvline(x=separation_point, color='red', linestyle='--', \n",
    "                              alpha=0.7, label='Fetched | Generated')\n",
    "            axes[1, 1].set_title('Token Perplexity Over Iterations')\n",
    "            axes[1, 1].set_xlabel('Iteration')\n",
    "            axes[1, 1].set_ylabel('Token Perplexity')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "            axes[1, 1].legend()\n",
    "        else:\n",
    "            axes[1, 1].text(0.5, 0.5, 'No perplexity data\\navailable', \n",
    "                           ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "            axes[1, 1].set_title('Token Perplexity Over Iterations')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Summary statistics\n",
    "        print(f\"\\nDataset Summary:\")\n",
    "        print(f\"  Shape: {df.shape}\")\n",
    "        print(f\"  Total iterations: {len(df)}\")\n",
    "        \n",
    "        # Show available metrics\n",
    "        if len(analysis_df.columns) > 0:\n",
    "            print(f\"\\nAvailable Analysis Metrics:\")\n",
    "            for col in analysis_df.columns:\n",
    "                non_null_count = analysis_df[col].notna().sum()\n",
    "                print(f\"  - {col}: {non_null_count} non-null values\")\n",
    "        \n",
    "        # Show numeric statistics for requested metrics\n",
    "        requested_cols = ['lexical_similarity', 'semantic_similarity', \n",
    "                         'semantic_similarity_window', 'token_perplexity']\n",
    "        available_cols = [col for col in requested_cols if col in df.columns and df[col].notna().any()]\n",
    "        \n",
    "        if available_cols:\n",
    "            print(f\"\\nRequested Metric Statistics:\")\n",
    "            print(df[available_cols].describe())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error examining output files: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
