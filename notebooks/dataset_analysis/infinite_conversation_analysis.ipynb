{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infinite Conversation Dataset Analysis\n",
    "\n",
    "This notebook analyzes the Infinite Conversation dataset, which contains simulated conversations between philosophers like Slavoj Zizek and Werner Herzog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the dataset\n",
    "data_dir = Path.cwd().parent.parent / 'data' / 'infinite_conversation'\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "\n",
    "# List all JSON files in the directory\n",
    "json_files = list(data_dir.glob('conversation_*.json'))\n",
    "print(f\"Found {len(json_files)} conversation files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a conversation file\n",
    "def load_conversation(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Load all conversations\n",
    "conversations = []\n",
    "for file_path in json_files:\n",
    "    conv_id = file_path.stem  # Get filename without extension\n",
    "    data = load_conversation(file_path)\n",
    "    conversations.append({\n",
    "        'id': conv_id,\n",
    "        'data': data\n",
    "    })\n",
    "\n",
    "print(f\"Loaded {len(conversations)} conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First conversation data:\")\n",
    "print(f\"ID: {conversations[0]['id']}\")\n",
    "print(\"\\nRaw data:\")\n",
    "for key, value in conversations[0]['data'].items():\n",
    "    print(f\"{key}: {value[:79]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract messages from a conversation\n",
    "def extract_messages(conversation_data):\n",
    "    messages = []\n",
    "    for key, value in conversation_data.items():\n",
    "        # Extract speaker name from the text\n",
    "        if ':' in value:\n",
    "            speaker, text = value.split(':', 1)\n",
    "        else:\n",
    "            speaker = \"Unknown\"\n",
    "            text = value\n",
    "        \n",
    "        messages.append({\n",
    "            'key': key,  # MP3 filename\n",
    "            'speaker': speaker.strip(),\n",
    "            'text': text.strip()\n",
    "        })\n",
    "    \n",
    "    # Extract timestamp from keys like \"/slavoj_1704311301.66552.mp3\"\n",
    "    def get_timestamp(item):\n",
    "        key = item['key']\n",
    "        parts = key.split('_')\n",
    "        if len(parts) > 1 and '.' in parts[1]:\n",
    "            timestamp_parts = parts[1].split('.')\n",
    "            if len(timestamp_parts) >= 2:\n",
    "                return float(timestamp_parts[0] + '.' + timestamp_parts[1])\n",
    "        return 0\n",
    "    \n",
    "    # Sort messages by extracted timestamp\n",
    "    messages.sort(key=get_timestamp)\n",
    "    \n",
    "    return messages\n",
    "\n",
    "# Process all conversations\n",
    "for conv in conversations:\n",
    "    conv['messages'] = extract_messages(conv['data'])\n",
    "    print(f\"Conversation {conv['id']}: {len(conv['messages'])} messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample conversation\n",
    "sample_conv = conversations[0]\n",
    "print(f\"Sample conversation: {sample_conv['id']}\\n\")\n",
    "print(f\"{'='*79}\")\n",
    "\n",
    "for msg in sample_conv['messages']:\n",
    "    print(f\"{msg['speaker']}: {msg['text'][:100]}...\")\n",
    "    print(f\"{'-'*79}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format conversation for display\n",
    "def format_conversation(messages):\n",
    "    formatted = []\n",
    "    for msg in messages:\n",
    "        formatted.append(f\"{msg['speaker']}: {msg['text']}\")\n",
    "    return \"\\n\\n\".join(formatted)\n",
    "\n",
    "# Display the first conversation fully\n",
    "print(f\"\\n{'='*79}\")\n",
    "print(f\"Full Conversation: {conversations[0]['id']}\")\n",
    "print(f\"{'='*79}\")\n",
    "print(format_conversation(conversations[0]['messages']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SemanticAnalyzer\n",
    "from src.babel_ai.analyzer import SimilarityAnalyzer\n",
    "\n",
    "# Initialize the SemanticAnalyzer\n",
    "analyzer = SimilarityAnalyzer(\n",
    "    analyze_window=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract just the text for analysis\n",
    "def extract_conversation_text(messages):\n",
    "    return [msg['text'] for msg in messages]\n",
    "\n",
    "# Analyze each conversation\n",
    "results = []\n",
    "for conv in conversations:\n",
    "    print(f\"\\nAnalyzing conversation {conv['id']} with {len(conv['messages'])} messages...\")\n",
    "    \n",
    "    # Extract text messages\n",
    "    messages = extract_conversation_text(conv['messages'])\n",
    "    \n",
    "    # Analyze each message\n",
    "    metrics = []\n",
    "    for i, message in enumerate(messages):\n",
    "        analysis = analyzer.analyze(messages[:i+1])\n",
    "        metrics.append({\n",
    "            'iteration': i,\n",
    "            'message': message,\n",
    "            'analysis': analysis\n",
    "        })\n",
    "    \n",
    "    results.append({\n",
    "        'conversation_id': conv['id'],\n",
    "        'metrics': metrics\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results for each conversation\n",
    "for result in results:\n",
    "    conv_id = result['conversation_id']\n",
    "    metrics = result['metrics']\n",
    "    \n",
    "    # Create figure and axis objects\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Extract data\n",
    "    iterations = [m['iteration'] for m in metrics]\n",
    "    lexical_sim = [m['analysis']['lexical_similarity'] \n",
    "                   if 'lexical_similarity' in m['analysis'] else None \n",
    "                   for m in metrics]\n",
    "    semantic_sim = [m['analysis']['semantic_similarity']\n",
    "                    if 'semantic_similarity' in m['analysis'] else None\n",
    "                    for m in metrics]\n",
    "    semantic_surp = [m['analysis']['semantic_surprise']\n",
    "                     if 'semantic_surprise' in m['analysis'] else None\n",
    "                     for m in metrics]\n",
    "    \n",
    "    # Remove None values\n",
    "    valid_indices = [i for i, v in enumerate(lexical_sim) if v is not None]\n",
    "    iterations = [iterations[i] for i in valid_indices]\n",
    "    lexical_sim = [v for v in lexical_sim if v is not None]\n",
    "    semantic_sim = [v for v in semantic_sim if v is not None]\n",
    "    semantic_surp = [v for v in semantic_surp if v is not None]\n",
    "    \n",
    "    # Plot similarities\n",
    "    ax1.set_xlabel('Message Index')\n",
    "    ax1.set_ylabel('Similarity Score', color='tab:blue')\n",
    "    ax1.plot(iterations, lexical_sim, label='Lexical Similarity',\n",
    "             marker='o', color='tab:blue', alpha=0.6)\n",
    "    ax1.plot(iterations, semantic_sim, label='Semantic Similarity',\n",
    "             marker='s', color='tab:orange', alpha=0.6)\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    \n",
    "    # Plot surprise on second y-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Surprise Score', color='tab:red')\n",
    "    ax2.plot(iterations, semantic_surp, label='Semantic Surprise',\n",
    "             marker='^', color='tab:red', alpha=0.6)\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "    \n",
    "    # Add legends\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "    \n",
    "    # Add grid\n",
    "    ax1.grid(True, which='major', linestyle='-', alpha=0.5)\n",
    "    ax1.grid(True, which='minor', linestyle=':', alpha=0.2)\n",
    "    \n",
    "    plt.title(f'Analysis of Conversation {conv_id} ({len(metrics)} messages)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
