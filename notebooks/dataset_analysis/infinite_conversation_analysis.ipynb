{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infinite Conversation Dataset Analysis\n",
    "\n",
    "This notebook analyzes the Infinite Conversation dataset, which contains simulated conversations between philosophers like Slavoj Zizek and Werner Herzog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "sys.path.append('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the dataset\n",
    "data_dir = Path.cwd().parent.parent / 'data' / 'infinite_conversation'\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "\n",
    "# List all JSON files in the directory\n",
    "json_files = list(data_dir.glob('conversation_*.json'))\n",
    "print(f\"Found {len(json_files)} conversation files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a conversation file\n",
    "def load_conversation(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Load all conversations\n",
    "conversations = []\n",
    "for file_path in json_files:\n",
    "    conv_id = file_path.stem  # Get filename without extension\n",
    "    data = load_conversation(file_path)\n",
    "    conversations.append({\n",
    "        'id': conv_id,\n",
    "        'data': data\n",
    "    })\n",
    "\n",
    "print(f\"Loaded {len(conversations)} conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First conversation data:\")\n",
    "print(f\"ID: {conversations[0]['id']}\")\n",
    "print(\"\\nRaw data:\")\n",
    "for key, value in conversations[0]['data'].items():\n",
    "    print(f\"{key}: {value[:79]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract messages from a conversation\n",
    "def extract_messages(conversation_data):\n",
    "    messages = []\n",
    "    for key, value in conversation_data.items():\n",
    "        # Extract speaker name from the text\n",
    "        if ':' in value:\n",
    "            speaker, text = value.split(':', 1)\n",
    "        else:\n",
    "            speaker = \"Unknown\"\n",
    "            text = value\n",
    "        \n",
    "        messages.append({\n",
    "            'key': key,  # MP3 filename\n",
    "            'speaker': speaker.strip(),\n",
    "            'text': text.strip()\n",
    "        })\n",
    "    \n",
    "    # Extract timestamp from keys like \"/slavoj_1704311301.66552.mp3\"\n",
    "    def get_timestamp(item):\n",
    "        key = item['key']\n",
    "        parts = key.split('_')\n",
    "        if len(parts) > 1 and '.' in parts[1]:\n",
    "            timestamp_parts = parts[1].split('.')\n",
    "            if len(timestamp_parts) >= 2:\n",
    "                return float(timestamp_parts[0] + '.' + timestamp_parts[1])\n",
    "        return 0\n",
    "    \n",
    "    # Sort messages by extracted timestamp\n",
    "    messages.sort(key=get_timestamp)\n",
    "    \n",
    "    return messages\n",
    "\n",
    "# Process all conversations\n",
    "for conv in conversations:\n",
    "    conv['messages'] = extract_messages(conv['data'])\n",
    "    print(f\"Conversation {conv['id']}: {len(conv['messages'])} messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample conversation\n",
    "sample_conv = conversations[0]\n",
    "print(f\"Sample conversation: {sample_conv['id']}\\n\")\n",
    "print(f\"{'='*79}\")\n",
    "\n",
    "for msg in sample_conv['messages']:\n",
    "    print(f\"{msg['speaker']}: {msg['text'][:100]}...\")\n",
    "    print(f\"{'-'*79}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format conversation for display\n",
    "def format_conversation(messages):\n",
    "    formatted = []\n",
    "    for msg in messages:\n",
    "        formatted.append(f\"{msg['speaker']}: {msg['text']}\")\n",
    "    return \"\\n\\n\".join(formatted)\n",
    "\n",
    "# Display the first conversation fully\n",
    "print(f\"\\n{'='*79}\")\n",
    "print(f\"Full Conversation: {conversations[0]['id']}\")\n",
    "print(f\"{'='*79}\")\n",
    "print(format_conversation(conversations[0]['messages']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SemanticAnalyzer\n",
    "from src.babel_ai.analyzer import SimilarityAnalyzer\n",
    "\n",
    "# Initialize the SemanticAnalyzer\n",
    "analyzer = SimilarityAnalyzer(\n",
    "    analyze_window=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract just the text for analysis\n",
    "def extract_conversation_text(messages):\n",
    "    return [msg['text'] for msg in messages]\n",
    "\n",
    "# Analyze each conversation\n",
    "results = []\n",
    "for conv in conversations:\n",
    "    print(f\"\\nAnalyzing conversation {conv['id']} with {len(conv['messages'])} messages...\")\n",
    "    \n",
    "    # Extract text messages\n",
    "    messages = extract_conversation_text(conv['messages'])\n",
    "    \n",
    "    # Analyze each message\n",
    "    metrics = []\n",
    "    for i, message in enumerate(messages):\n",
    "        analysis = analyzer.analyze(messages[:i+1])\n",
    "        metrics.append({\n",
    "            'iteration': i,\n",
    "            'message': message,\n",
    "            'analysis': analysis\n",
    "        })\n",
    "    \n",
    "    results.append({\n",
    "        'conversation_id': conv['id'],\n",
    "        'metrics': metrics\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results for each conversation\n",
    "for result in results:\n",
    "    conv_id = result['conversation_id']\n",
    "    metrics = result['metrics']\n",
    "    \n",
    "    # Create figure and axis objects\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Extract data\n",
    "    iterations = [m['iteration'] for m in metrics]\n",
    "    lexical_sim = [m['analysis']['lexical_similarity'] \n",
    "                   if 'lexical_similarity' in m['analysis'] else None \n",
    "                   for m in metrics]\n",
    "    semantic_sim = [m['analysis']['semantic_similarity']\n",
    "                    if 'semantic_similarity' in m['analysis'] else None\n",
    "                    for m in metrics]\n",
    "    semantic_surp = [m['analysis']['semantic_surprise']\n",
    "                     if 'semantic_surprise' in m['analysis'] else None\n",
    "                     for m in metrics]\n",
    "    \n",
    "    # Remove None values\n",
    "    valid_indices = [i for i, v in enumerate(lexical_sim) if v is not None]\n",
    "    iterations = [iterations[i] for i in valid_indices]\n",
    "    lexical_sim = [v for v in lexical_sim if v is not None]\n",
    "    semantic_sim = [v for v in semantic_sim if v is not None]\n",
    "    semantic_surp = [v for v in semantic_surp if v is not None]\n",
    "    \n",
    "    # Plot similarities\n",
    "    ax1.set_xlabel('Message Index')\n",
    "    ax1.set_ylabel('Similarity Score', color='tab:blue')\n",
    "    ax1.plot(iterations, lexical_sim, label='Lexical Similarity',\n",
    "             marker='o', color='tab:blue', alpha=0.6)\n",
    "    ax1.plot(iterations, semantic_sim, label='Semantic Similarity',\n",
    "             marker='s', color='tab:orange', alpha=0.6)\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    \n",
    "    # Plot surprise on second y-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Surprise Score', color='tab:red')\n",
    "    ax2.plot(iterations, semantic_surp, label='Semantic Surprise',\n",
    "             marker='^', color='tab:red', alpha=0.6)\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "    \n",
    "    # Add legends\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "    \n",
    "    # Add grid\n",
    "    ax1.grid(True, which='major', linestyle='-', alpha=0.5)\n",
    "    ax1.grid(True, which='minor', linestyle=':', alpha=0.2)\n",
    "    \n",
    "    plt.title(f'Analysis of Conversation {conv_id} ({len(metrics)} messages)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Conversation Fragments into a Continuous Conversation\n",
    "\n",
    "According to our observation, the conversation files appear to be fragments of a longer, continuous conversation, with each fragment ending with the same message that the next one starts with. In this section, we'll:\n",
    "\n",
    "1. Identify the correct sequence of conversation fragments\n",
    "2. Combine them into a single continuous conversation\n",
    "3. Remove duplicated messages at the fragment boundaries\n",
    "4. Analyze and plot the resulting continuous conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the conversations into a single continuous conversation\n",
    "print(\"Combining conversations into a continuous sequence...\")\n",
    "sequence = conversations\n",
    "# Sort conversations based on the timestamp in the first message key\n",
    "sequence = sorted(sequence, key=lambda conv: conv['id'])\n",
    "\n",
    "continuous_conversation = []\n",
    "\n",
    "for i, conv in enumerate(sequence):\n",
    "    if i == 0:\n",
    "        # Include all messages from the first conversation\n",
    "        continuous_conversation.extend(conv['messages'])\n",
    "    else:\n",
    "        # Skip the first message of subsequent conversations (it's a duplicate)\n",
    "        continuous_conversation.extend(conv['messages'][1:])\n",
    "\n",
    "print(f\"Combined conversation has {len(continuous_conversation)} messages\")\n",
    "\n",
    "# Verify there are no duplicated consecutive messages\n",
    "duplicates = 0\n",
    "for i in range(1, len(continuous_conversation)):\n",
    "    prev_msg = continuous_conversation[i-1]\n",
    "    curr_msg = continuous_conversation[i]\n",
    "    \n",
    "    if prev_msg['text'] == curr_msg['text'] and prev_msg['speaker'] == curr_msg['speaker']:\n",
    "        duplicates += 1\n",
    "        print(f\"Found duplicate: {prev_msg['speaker']}: {prev_msg['text'][:50]}...\")\n",
    "\n",
    "print(f\"Found {duplicates} duplicated consecutive messages\")\n",
    "\n",
    "# Display the beginning and end of the continuous conversation\n",
    "print(\"\\nFirst 3 messages of the continuous conversation:\")\n",
    "for msg in continuous_conversation[:3]:\n",
    "    print(f\"{msg['speaker']}: {msg['text'][:79]}...\")\n",
    "    print(f\"{'-'*79}\")\n",
    "\n",
    "print(\"\\nLast 3 messages of the continuous conversation:\")\n",
    "for msg in continuous_conversation[-3:]:\n",
    "    print(f\"{msg['speaker']}: {msg['text'][:79]}...\")\n",
    "    print(f\"{'-'*79}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the full continuous conversation\n",
    "print(\"Full continuous conversation:\")\n",
    "print(\"=\"*79)\n",
    "for msg in continuous_conversation:\n",
    "    # Print speaker and message text, wrapping at 79 chars\n",
    "    print(f\"{msg['speaker']}:\")\n",
    "    \n",
    "    # Split text into 79-char chunks for readability\n",
    "    text = msg['text']\n",
    "    while text:\n",
    "        if len(text) <= 79:\n",
    "            print(text)\n",
    "            text = \"\"\n",
    "        else:\n",
    "            # Find last space before 79 chars\n",
    "            split_point = text[:79].rfind(' ')\n",
    "            if split_point == -1:\n",
    "                split_point = 79\n",
    "            print(text[:split_point])\n",
    "            text = text[split_point:].lstrip()\n",
    "    \n",
    "    print(\"-\"*79)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the continuous conversation\n",
    "print(\"Analyzing the continuous conversation...\")\n",
    "\n",
    "# Extract text messages for analysis\n",
    "continuous_messages = extract_conversation_text(continuous_conversation)\n",
    "\n",
    "# Analyze each message\n",
    "continuous_metrics = []\n",
    "for i, message in enumerate(continuous_messages):\n",
    "    if i % 50 == 0:\n",
    "        print(f\"Analyzing message {i+1}/{len(continuous_messages)}...\")\n",
    "    analysis = analyzer.analyze(continuous_messages[:i+1])\n",
    "    continuous_metrics.append({\n",
    "        'iteration': i,\n",
    "        'message': message,\n",
    "        'analysis': analysis\n",
    "    })\n",
    "\n",
    "# Add the continuous conversation result\n",
    "continuous_result = {\n",
    "    'conversation_id': 'continuous_conversation',\n",
    "    'metrics': continuous_metrics\n",
    "}\n",
    "\n",
    "print(f\"Completed analysis of {len(continuous_metrics)} messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results for the continuous conversation\n",
    "# Create figure and axis objects\n",
    "fig, ax1 = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "# Extract data\n",
    "iterations = [m['iteration'] for m in continuous_metrics]\n",
    "lexical_sim = [m['analysis']['lexical_similarity'] \n",
    "               if 'lexical_similarity' in m['analysis'] else None \n",
    "               for m in continuous_metrics]\n",
    "semantic_sim = [m['analysis']['semantic_similarity']\n",
    "                if 'semantic_similarity' in m['analysis'] else None\n",
    "                for m in continuous_metrics]\n",
    "semantic_surp = [m['analysis']['semantic_surprise']\n",
    "                 if 'semantic_surprise' in m['analysis'] else None\n",
    "                 for m in continuous_metrics]\n",
    "\n",
    "# Remove None values\n",
    "valid_indices = [i for i, v in enumerate(lexical_sim) if v is not None]\n",
    "iterations = [iterations[i] for i in valid_indices]\n",
    "lexical_sim = [v for v in lexical_sim if v is not None]\n",
    "semantic_sim = [v for v in semantic_sim if v is not None]\n",
    "semantic_surp = [v for v in semantic_surp if v is not None]\n",
    "\n",
    "# Plot similarities\n",
    "ax1.set_xlabel('Message Index')\n",
    "ax1.set_ylabel('Similarity Score', color='tab:blue')\n",
    "ax1.plot(iterations, lexical_sim, label='Lexical Similarity',\n",
    "         marker='o', color='tab:blue', alpha=0.6, markersize=3)\n",
    "ax1.plot(iterations, semantic_sim, label='Semantic Similarity',\n",
    "         marker='s', color='tab:orange', alpha=0.6, markersize=3)\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Mark fragment boundaries with vertical lines\n",
    "boundary_indices = []\n",
    "msg_count = 0\n",
    "for conv in sequence[:-1]:  # All except the last conversation\n",
    "    msg_count += len(conv['messages'])\n",
    "    boundary_indices.append(msg_count - 1)  # -1 because we're 0-indexed\n",
    "\n",
    "for idx in boundary_indices:\n",
    "    if idx in iterations:\n",
    "        ax1.axvline(x=idx, color='gray', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot surprise on second y-axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Surprise Score', color='tab:red')\n",
    "ax2.plot(iterations, semantic_surp, label='Semantic Surprise',\n",
    "         marker='^', color='tab:red', alpha=0.6, markersize=3)\n",
    "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "# Add legends\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "\n",
    "# Add grid\n",
    "ax1.grid(True, which='major', linestyle='-', alpha=0.5)\n",
    "ax1.grid(True, which='minor', linestyle=':', alpha=0.2)\n",
    "\n",
    "plt.title('Analysis of Combined Continuous Conversation')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot rolling average of semantic similarity and surprise\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "# Calculate rolling averages\n",
    "window_size = min(20, len(lexical_sim) // 10)  # Adaptive window size\n",
    "if window_size < 3:\n",
    "    window_size = 3  # Minimum window size\n",
    "if window_size % 2 == 0:\n",
    "    window_size += 1  # Ensure odd window size for Savitzky-Golay\n",
    "\n",
    "# Use Savitzky-Golay filter for smoothing\n",
    "poly_order = min(3, window_size - 1)  # Polynomial order must be less than window size\n",
    "smoothed_lexical = savgol_filter(lexical_sim, window_size, poly_order)\n",
    "smoothed_semantic = savgol_filter(semantic_sim, window_size, poly_order)\n",
    "smoothed_surprise = savgol_filter(semantic_surp, window_size, poly_order)\n",
    "\n",
    "# Create the plot\n",
    "fig, ax1 = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "# Plot smoothed similarities\n",
    "ax1.set_xlabel('Message Index')\n",
    "ax1.set_ylabel('Smoothed Similarity Score', color='tab:blue')\n",
    "ax1.plot(iterations, smoothed_lexical, label='Smoothed Lexical Similarity',\n",
    "         color='tab:blue', linewidth=2)\n",
    "ax1.plot(iterations, smoothed_semantic, label='Smoothed Semantic Similarity',\n",
    "         color='tab:orange', linewidth=2)\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Mark fragment boundaries with vertical lines\n",
    "for idx in boundary_indices:\n",
    "    if idx in iterations:\n",
    "        ax1.axvline(x=idx, color='gray', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot smoothed surprise on second y-axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Smoothed Surprise Score', color='tab:red')\n",
    "ax2.plot(iterations, smoothed_surprise, label='Smoothed Semantic Surprise',\n",
    "         color='tab:red', linewidth=2)\n",
    "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "# Add legends\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "\n",
    "# Add grid\n",
    "ax1.grid(True, which='major', linestyle='-', alpha=0.5)\n",
    "ax1.grid(True, which='minor', linestyle=':', alpha=0.2)\n",
    "\n",
    "plt.title(f'Smoothed Analysis of Combined Conversation (Window Size: {window_size})')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings from the Continuous Conversation Analysis\n",
    "\n",
    "1. **Message Distribution**: The combined conversation contains messages across multiple fragments, with vertical lines in the plot marking the boundaries between original conversation files.\n",
    "\n",
    "2. **Similarity Patterns**: The smoothed plots reveal trends in lexical and semantic similarity that might be obscured in the individual fragments.\n",
    "\n",
    "3. **Surprise Metrics**: The semantic surprise metric shows how unexpected each message is compared to previous ones. High surprise values indicate topic shifts or unexpected statements in the conversation.\n",
    "\n",
    "4. **Conversation Flow**: Observing the metrics across the full conversation provides insights into how the dialogue progresses and evolves over time, potentially revealing patterns that weren't visible in the fragmented analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
