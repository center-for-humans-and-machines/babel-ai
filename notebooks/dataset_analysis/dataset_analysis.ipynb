{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ShareGPT Dataset Preview\n",
    "\n",
    "This notebook efficiently loads and previews the ShareGPT dataset using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the dataset\n",
    "data_path = Path.cwd().parent.parent / 'data' / 'human-ai_datasets' / 'sharegpt_clean.json'\n",
    "\n",
    "# Read only the first 12 lines efficiently\n",
    "with open(data_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "items = [d['items'] for d in data]\n",
    "items = [item for item in items if len(item) > 100]\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(items)\n",
    "\n",
    "# Display the first few rows\n",
    "print(f\"Total rows loaded: {len(df)}\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_conversation(row):\n",
    "    conversation = []\n",
    "    for i in range(len(row)):\n",
    "        if pd.isna(row[i]):\n",
    "            break\n",
    "        message = row[i]\n",
    "        if isinstance(message, str):\n",
    "            message = eval(message)  # Convert string representation of dict to dict\n",
    "        role = \"Human\" if message['from'] == 'human' else \"Assistant\"\n",
    "        conversation.append(f\"{role}: {message['value']}\")\n",
    "    return \"\\n\\n\".join(conversation)\n",
    "\n",
    "# Format the first 5 conversations\n",
    "for idx, row in df.head(5).iterrows():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Conversation {idx + 1}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(format_conversation(row))\n",
    "    print(f\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.babel_ai.analyzer import SimilarityAnalyzer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize the DriftAnalyzer\n",
    "analyzer = SimilarityAnalyzer()\n",
    "\n",
    "# Function to extract conversation text\n",
    "def extract_conversation_text(row):\n",
    "    messages = []\n",
    "    for i in range(len(row)):\n",
    "        if pd.isna(row[i]):\n",
    "            break\n",
    "        message = row[i]\n",
    "        if isinstance(message, str):\n",
    "            message = eval(message)  # Convert string representation of dict to dict\n",
    "        messages.append(message['value'])\n",
    "    return messages\n",
    "\n",
    "# Find conversations with more than 100 messages\n",
    "long_conversations = []\n",
    "for idx, row in df.iterrows():\n",
    "    msg_count = sum(1 for x in row if pd.notna(x))\n",
    "    if msg_count > 100:\n",
    "        long_conversations.append((idx, extract_conversation_text(row)))\n",
    "\n",
    "# Select 3 conversations to analyze\n",
    "selected_conversations = long_conversations[3:6]\n",
    "\n",
    "# Analyze each conversation\n",
    "results = []\n",
    "for conv_idx, messages in selected_conversations:\n",
    "    print(f\"\\nAnalyzing conversation {conv_idx} with {len(messages)} messages...\")\n",
    "    \n",
    "    # Analyze each message\n",
    "    metrics = []\n",
    "    for i, message in enumerate(messages):\n",
    "        analysis = analyzer.analyze(messages[:i+1])\n",
    "        metrics.append({\n",
    "            'iteration': i,\n",
    "            'message': message,\n",
    "            'analysis': analysis\n",
    "        })\n",
    "    \n",
    "    results.append({\n",
    "        'conversation_id': conv_idx,\n",
    "        'metrics': metrics\n",
    "    })\n",
    "\n",
    "# Plot results for each conversation\n",
    "for result in results:\n",
    "    conv_id = result['conversation_id']\n",
    "    metrics = result['metrics']\n",
    "    \n",
    "    # Create figure and axis objects\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Extract data\n",
    "    iterations = [m['iteration'] for m in metrics]\n",
    "    lexical_sim = [m['analysis']['lexical_similarity'] \n",
    "                   if 'lexical_similarity' in m['analysis'] else None \n",
    "                   for m in metrics]\n",
    "    semantic_sim = [m['analysis']['semantic_similarity']\n",
    "                    if 'semantic_similarity' in m['analysis'] else None\n",
    "                    for m in metrics]\n",
    "    semantic_surp = [m['analysis']['semantic_surprise']\n",
    "                     if 'semantic_surprise' in m['analysis'] else None\n",
    "                     for m in metrics]\n",
    "    \n",
    "    # Remove None values\n",
    "    valid_indices = [i for i, v in enumerate(lexical_sim) if v is not None]\n",
    "    iterations = [iterations[i] for i in valid_indices]\n",
    "    lexical_sim = [v for v in lexical_sim if v is not None]\n",
    "    semantic_sim = [v for v in semantic_sim if v is not None]\n",
    "    semantic_surp = [v for v in semantic_surp if v is not None]\n",
    "    \n",
    "    # Plot similarities\n",
    "    ax1.set_xlabel('Message Index')\n",
    "    ax1.set_ylabel('Similarity Score', color='tab:blue')\n",
    "    ax1.plot(iterations, lexical_sim, label='Lexical Similarity',\n",
    "             marker='o', color='tab:blue', alpha=0.6)\n",
    "    ax1.plot(iterations, semantic_sim, label='Semantic Similarity',\n",
    "             marker='s', color='tab:orange', alpha=0.6)\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    \n",
    "    # Plot surprise on second y-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Surprise Score', color='tab:red')\n",
    "    ax2.plot(iterations, semantic_surp, label='Semantic Surprise',\n",
    "             marker='^', color='tab:red', alpha=0.6)\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "    \n",
    "    # Add legends\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "    \n",
    "    # Add grid\n",
    "    ax1.grid(True, which='major', linestyle='-', alpha=0.5)\n",
    "    ax1.grid(True, which='minor', linestyle=':', alpha=0.2)\n",
    "    \n",
    "    plt.title(f'Analysis of Conversation {conv_id} ({len(metrics)} messages)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
