{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Initialize Raven Models\n",
    "\n",
    "This notebook initializes multiple models in the Raven backend using the `raven_ollama_request` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src directory to Python path\n",
    "src_path = str(Path.cwd().parent / \"src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "from api.ollama import OllamaModel, raven_ollama_request\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Define Models to Initialize\n",
    "\n",
    "Add or remove models from this list as needed. These are the available models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available models:\n",
      "['LLAMA3:llama3',\n",
      " 'LLAMA3_8B:llama3:8b',\n",
      " 'MISTRAL:mistral',\n",
      " 'MISTRAL_7B:mistral:7b-instruct',\n",
      " 'MISTRAL_7B_TEXT:mistral:7b-text',\n",
      " 'MIXTRAL:mixtral',\n",
      " 'LLAMA3_70B:llama3:70b',\n",
      " 'LLAMA3_70B_TEXT:llama3:70b-text',\n",
      " 'LLAMA33_70B:llama3.3:70b',\n",
      " 'DEEPSEEK_R1:deepseek-r1:70b',\n",
      " 'GPT_2_1_5B:gpt2:1.5b']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAvailable models:\")\n",
    "pprint([f\"{k}:{v.value}\" for k, v in OllamaModel.__members__.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models to initialize\n",
    "models_to_initialize = [\n",
    "    OllamaModel.LLAMA33_70B,  # Default Raven model\n",
    "    OllamaModel.MISTRAL_7B,   # Example additional model\n",
    "    # Add more models here\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Initialize Models\n",
    "\n",
    "This cell will attempt to initialize each model with a simple test prompt. It will track which models were successfully initialized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing llama3.3:70b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in Ollama API request: 500 Server Error: Internal Server Error for url: https://hpc-llm-inference-fastapi.chm.mpib-berlin.mpg.de/v1/chat/completions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully initialized llama3.3:70b\n",
      "Response: Model initialization test\n",
      "\n",
      "Initializing mistral:7b-instruct...\n",
      "✗ Failed to initialize mistral:7b-instruct - Error: 500 Server Error: Internal Server Error for url: https://hpc-llm-inference-fastapi.chm.mpib-berlin.mpg.de/v1/chat/completions\n"
     ]
    }
   ],
   "source": [
    "test_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Say 'Model initialization test'\"}\n",
    "]\n",
    "\n",
    "successful_models = []\n",
    "failed_models = []\n",
    "\n",
    "for model in models_to_initialize:\n",
    "    try:\n",
    "        print(f\"\\nInitializing {model.value}...\")\n",
    "        response = raven_ollama_request(\n",
    "            messages=test_messages,\n",
    "            model=model,\n",
    "            temperature=0.7,\n",
    "            max_tokens=50\n",
    "        )\n",
    "        if response and len(response) > 0:\n",
    "            print(f\"✓ Successfully initialized {model.value}\")\n",
    "            print(f\"Response: {response}\")\n",
    "            successful_models.append(model.value)\n",
    "        else:\n",
    "            print(f\"✗ Failed to initialize {model.value} - Empty response\")\n",
    "            failed_models.append(model.value)\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to initialize {model.value} - Error: {str(e)}\")\n",
    "        failed_models.append(model.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initialization Summary:\n",
      "Total models attempted: 2\n",
      "Successfully initialized: 1\n",
      "Failed to initialize: 1\n",
      "\n",
      "Successfully initialized models:\n",
      "- llama3.3:70b\n",
      "\n",
      "Failed to initialize models:\n",
      "- mistral:7b-instruct\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nInitialization Summary:\")\n",
    "print(f\"Total models attempted: {len(models_to_initialize)}\")\n",
    "print(f\"Successfully initialized: {len(successful_models)}\")\n",
    "print(f\"Failed to initialize: {len(failed_models)}\")\n",
    "\n",
    "if successful_models:\n",
    "    print(\"\\nSuccessfully initialized models:\")\n",
    "    for model in successful_models:\n",
    "        print(f\"- {model}\")\n",
    "\n",
    "if failed_models:\n",
    "    print(\"\\nFailed to initialize models:\")\n",
    "    for model in failed_models:\n",
    "        print(f\"- {model}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
