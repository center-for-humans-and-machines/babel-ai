{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the DriftAnalyzer\n",
    "\n",
    "This notebook demonstrates the functionality of the DriftAnalyzer class from the babel_ai project. We'll test various aspects of the analyzer including:\n",
    "- Word statistics analysis\n",
    "- Similarity analysis\n",
    "- Different types of inputs\n",
    "- Edge cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.babel_ai.llm_drift import DriftExperiment, ExperimentConfig\n",
    "from src.babel_ai.prompt_fetcher import ShareGPTPromptFetcher\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ExperimentConfig(\n",
    "    temperature=1.1,\n",
    "    max_tokens=2000,\n",
    "    frequency_penalty=2.0,\n",
    "    presence_penalty=2.0,\n",
    "    top_p=1.0,\n",
    "    max_iterations=4,\n",
    "    max_total_characters=1000000,\n",
    "    analyze_window=50,\n",
    ")\n",
    "\n",
    "drift_experiment = DriftExperiment(config=config)\n",
    "# initial_prompt= drift_experiment.fetch_prompt(category=\"creative\")\n",
    "metrics = drift_experiment.run(\n",
    "    initial_prompt='initial_prompt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([metric['analysis'] for metric in metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics from CSV file\n",
    "metrics_df = pd.read_csv('drift_experiment_20250312_163741.csv')\n",
    "\n",
    "# Convert DataFrame rows to list of dictionaries in same format as metrics\n",
    "metrics = []\n",
    "for _, row in metrics_df.iterrows():\n",
    "    metric = {\n",
    "        'iteration': row['iteration'],\n",
    "        'timestamp': row['timestamp'],\n",
    "        'response': row['response'],\n",
    "        'analysis': {\n",
    "            'word_count': row['word_count'],\n",
    "            'unique_word_count': row['unique_word_count'], \n",
    "            'coherence_score': row['coherence_score'],\n",
    "            'is_repetitive': row['is_repetitive'],\n",
    "            'lexical_similarity': row['lexical_similarity'],\n",
    "            'semantic_similarity': row['semantic_similarity'],\n",
    "            'semantic_surprise': row['semantic_surprise'],\n",
    "            'max_semantic_surprise': row['max_semantic_surprise'],\n",
    "            'is_surprising': row['is_surprising']\n",
    "        },\n",
    "        'config': {\n",
    "            'temperature': row['temperature'],\n",
    "            'max_tokens': row['max_tokens'],\n",
    "            'frequency_penalty': row['frequency_penalty'],\n",
    "            'presence_penalty': row['presence_penalty'],\n",
    "            'top_p': row['top_p'],\n",
    "            'max_iterations': row['max_iterations'],\n",
    "            'max_total_characters': row['max_total_characters']\n",
    "        }\n",
    "    }\n",
    "    metrics.append(metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = Path.cwd().parent / \"data/llm_only_data/drift_experiment_20250317_155440.csv\"\n",
    "\n",
    "# Load metrics from CSV file\n",
    "metrics_df = pd.read_csv(file_name)\n",
    "\n",
    "# Convert DataFrame rows to list of dictionaries in same format as metrics\n",
    "metrics = []\n",
    "for _, row in metrics_df.iterrows():\n",
    "    metric = {\n",
    "        'iteration': row['iteration'],\n",
    "        'timestamp': row['timestamp'],\n",
    "        'response': row['response'],\n",
    "        'analysis': {\n",
    "            'word_count': row['word_count'],\n",
    "            'unique_word_count': row['unique_word_count'], \n",
    "            'coherence_score': row['coherence_score'],\n",
    "            'is_repetitive': row['is_repetitive'],\n",
    "            'lexical_similarity': row['lexical_similarity'],\n",
    "            'semantic_similarity': row['semantic_similarity'],\n",
    "            'semantic_surprise': row['semantic_surprise'],\n",
    "            'max_semantic_surprise': row['max_semantic_surprise'],\n",
    "            'is_surprising': row['is_surprising']\n",
    "        },\n",
    "    }\n",
    "    metrics.append(metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure and axis objects with a single subplot\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Extract data from metrics\n",
    "iterations = [m['iteration'] for m in metrics]\n",
    "lexical_sim = [m['analysis']['lexical_similarity'] \n",
    "               if 'lexical_similarity' in m['analysis'] else None \n",
    "               for m in metrics]\n",
    "semantic_sim = [m['analysis']['semantic_similarity']\n",
    "                if 'semantic_similarity' in m['analysis'] else None\n",
    "                for m in metrics]\n",
    "semantic_surp = [m['analysis']['semantic_surprise']\n",
    "                 if 'semantic_surprise' in m['analysis'] else None\n",
    "                 for m in metrics]\n",
    "\n",
    "# Remove None values (from first iteration)\n",
    "valid_indices = [i for i, v in enumerate(lexical_sim) if v is not None]\n",
    "iterations = [iterations[i] for i in valid_indices]\n",
    "lexical_sim = [v for v in lexical_sim if v is not None]\n",
    "semantic_sim = [v for v in semantic_sim if v is not None]\n",
    "semantic_surp = [v for v in semantic_surp if v is not None]\n",
    "\n",
    "# Plot similarities on the first y-axis\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Similarity Score', color='tab:blue')\n",
    "ax1.plot(iterations, lexical_sim, label='Lexical Similarity',\n",
    "         marker='o', color='tab:blue', alpha=0.6)\n",
    "ax1.plot(iterations, semantic_sim, label='Semantic Similarity',\n",
    "         marker='s', color='tab:orange', alpha=0.6)\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "ax1.set_ylim(0, 1)  # Set y-axis limits for similarities\n",
    "\n",
    "# Create a second y-axis for surprise\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Surprise Score', color='tab:red')\n",
    "ax2.plot(iterations, semantic_surp, label='Semantic Surprise',\n",
    "         marker='^', color='tab:red', alpha=0.6)\n",
    "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "# Add both legends\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "\n",
    "# Set finer x-axis ticks every 5 iterations\n",
    "max_iter = max(iterations)\n",
    "ax1.set_xticks(range(0, max_iter + 1, 5))\n",
    "ax1.set_xticks(range(0, max_iter + 1), minor=True)\n",
    "\n",
    "# Add grid with major and minor lines\n",
    "ax1.grid(True, which='major', linestyle='-', alpha=0.5)\n",
    "ax1.grid(True, which='minor', linestyle=':', alpha=0.2)\n",
    "\n",
    "plt.title(f\"{file_name}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(\" \".join([metric[\"response\"] for metric in metrics]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the ShareGPT prompt fetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "cwd = Path.cwd()\n",
    "\n",
    "config = ExperimentConfig(\n",
    "    temperature=0.7,\n",
    "    max_tokens=2000,\n",
    "    frequency_penalty=2.0,\n",
    "    presence_penalty=2.0,\n",
    "    top_p=1.0,\n",
    "    max_iterations=50,\n",
    "    max_total_characters=1000000,\n",
    "    analyze_window=50,\n",
    ")\n",
    "\n",
    "drift_experiment = DriftExperiment(\n",
    "    config=config,\n",
    "    prompt_fetcher=ShareGPTPromptFetcher(\n",
    "        data_path=cwd.parent / 'data' / 'human-ai_datasets' / 'sharegpt_clean.json',\n",
    "        min_messages=50,\n",
    "        max_messages=100,\n",
    "    )\n",
    ")\n",
    "\n",
    "initial_prompt= drift_experiment.prompt_fetcher.get_random_prompt()\n",
    "metrics = drift_experiment.run(\n",
    "    initial_messages=initial_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"Whatever\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
